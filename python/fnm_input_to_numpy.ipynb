{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pandas in /usr/local/lib/python3.5/dist-packages (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /usr/local/lib/python3.5/dist-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (1.14.5)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.5/dist-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertAcq(acq_filename, includeLoanID = False):\n",
    "    acq = pd.read_parquet(acq_filename).sort_values('loan_id').reset_index(drop=True)\n",
    "    acq = acq.fillna(0)\n",
    "    if includeLoanID:\n",
    "        acq_numpy = acq[['loan_id', 'acq_def_ind', 'state_id', 'purpose_id', 'mi_type_id', \\\n",
    "                    'occupancy_status_id', 'product_type_id', 'property_type_id', \\\n",
    "                        'seller_id', 'zip3_id']].to_numpy()\n",
    "    else:\n",
    "        acq_numpy = acq[[           'acq_def_ind', 'state_id', 'purpose_id', 'mi_type_id', \\\n",
    "            'occupancy_status_id', 'product_type_id', 'property_type_id', \\\n",
    "                'seller_id', 'zip3_id']].to_numpy(dtype=np.int32)\n",
    "    return acq_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertSeq(files, includeLoanID = False):\n",
    "    # mapping of index or loan id to chunk and sequence offsets\n",
    "    lid_to_seq_idx =  []     \n",
    "    seq_numpy = [None] * len(files)\n",
    "    seq_fnames = { int(files[i].split('/')[-1].split('.')[0].split('_')[-1]): files[i] for i in range(len(files))}\n",
    "\n",
    "    for i, fname in seq_fnames.items():\n",
    "        print('processing name: {}'.format(fname))\n",
    "        seq = pd.read_parquet(fname)\n",
    "        seq = seq[seq.dlq <= 6 + 12] # one year \n",
    "        seq = seq.sort_values(['loan_id', 'yyyymm']).reset_index(drop=True)\n",
    "        lid = seq.loan_id.to_numpy()\n",
    "        lid_idx = np.concatenate((np.array([0]), np.where(lid[:-1]!=lid[1:])[0]+1, np.array([len(lid)])))\n",
    "\n",
    "        lid_to_seq_idx.append(pd.DataFrame({'loan_id':lid[lid_idx[:-1]], 'chunk_id': i, 'seq_idx_begin':lid_idx[:-1], 'seq_idx_end':lid_idx[1:]}))\n",
    "        \n",
    "        if includeLoanID:\n",
    "            seq_numpy[i] = seq[['loan_id', 'default_1y', 'yyyymm', 'dlq_adj', 'age', 'int_rate', 'current_upb_norm']].to_numpy(dtype=np.float64)\n",
    "        else:\n",
    "            seq_numpy[i] = seq[[           'default_1y', 'yyyymm', 'dlq_adj', 'age', 'int_rate', 'current_upb_norm']].to_numpy(dtype=np.float32)\n",
    "        del seq\n",
    "        gc.collect()\n",
    "    \n",
    "    print('concatenating lid_to_seq_idx')\n",
    "    lid_to_seq_idx = pd.concat(lid_to_seq_idx).sort_values('loan_id').set_index('loan_id', drop='True')\n",
    "    \n",
    "    return lid_to_seq_idx, seq_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDataset(data_path):\n",
    "    acquistion_fname = '/fnm_input_acq_parquet'\n",
    "    sequence_fname = '/fnm_input_seq_parquet*'\n",
    "\n",
    "    acquisition_nname = '/fnm_input_acq.npy'\n",
    "    sequence_nname = '/fnm_input_seq_{}.npy'\n",
    "    idx_to_seq_nname = '/fnm_input_idx_to_seq.npy'\n",
    "\n",
    "    print('Data path: {}'.format(data_path))\n",
    "    print('Acquistion parquet: {}'.format(data_path + acquistion_fname))\n",
    "    print('Sequence parquet: {}'.format(data_path + sequence_fname))\n",
    "\n",
    "    seq_files = sorted([f for f in glob.glob(data_path + sequence_fname, recursive=False)])\n",
    "    for f in seq_files:\n",
    "        print('\\tSequence chunk found: {}'.format(f))\n",
    "\n",
    "    print('Acquisition numpy: {}'.format(data_path + acquisition_nname))\n",
    "    print('Sequence numpy: {}'.format(data_path + sequence_nname))\n",
    "    print('Index to Sequence Index numpy: {}'.format(data_path + idx_to_seq_nname))\n",
    "    \n",
    "    acq_numpy = convertAcq(data_path + acquistion_fname)\n",
    "    lid_to_seq_idx, seq_numpy = convertSeq(seq_files, includeLoanID = False)\n",
    "    idx_to_seq = lid_to_seq_idx[['chunk_id', 'seq_idx_begin', 'seq_idx_end']].to_numpy()\n",
    "    \n",
    "    np.save(data_path + acquisition_nname, acq_numpy, allow_pickle=False, fix_imports=False)\n",
    "    np.save(data_path + idx_to_seq_nname, idx_to_seq, allow_pickle=False, fix_imports=False)\n",
    "    for chunk_idx, seq_numpy_chunk in enumerate(seq_numpy):\n",
    "        np.save(data_path + sequence_nname.format(chunk_idx), seq_numpy[chunk_idx], allow_pickle=False, fix_imports=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: /home/user/notebooks/data/test\n",
      "Acquistion parquet: /home/user/notebooks/data/test/fnm_input_acq_parquet\n",
      "Sequence parquet: /home/user/notebooks/data/test/fnm_input_seq_parquet*\n",
      "\tSequence chunk found: /home/user/notebooks/data/test/fnm_input_seq_parquet_0\n",
      "Acquisition numpy: /home/user/notebooks/data/test/fnm_input_acq.npy\n",
      "Sequence numpy: /home/user/notebooks/data/test/fnm_input_seq_{}.npy\n",
      "Index to Sequence Index numpy: /home/user/notebooks/data/test/fnm_input_idx_to_seq.npy\n",
      "processing name: /home/user/notebooks/data/test/fnm_input_seq_parquet_0\n",
      "concatenating lid_to_seq_idx\n"
     ]
    }
   ],
   "source": [
    "convertDataset(data_path = '/home/user/notebooks/data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: /home/user/notebooks/data/train\n",
      "Acquistion parquet: /home/user/notebooks/data/train/fnm_input_acq_parquet\n",
      "Sequence parquet: /home/user/notebooks/data/train/fnm_input_seq_parquet*\n",
      "\tSequence chunk found: /home/user/notebooks/data/train/fnm_input_seq_parquet_0\n",
      "\tSequence chunk found: /home/user/notebooks/data/train/fnm_input_seq_parquet_1\n",
      "\tSequence chunk found: /home/user/notebooks/data/train/fnm_input_seq_parquet_2\n",
      "\tSequence chunk found: /home/user/notebooks/data/train/fnm_input_seq_parquet_3\n",
      "\tSequence chunk found: /home/user/notebooks/data/train/fnm_input_seq_parquet_4\n",
      "\tSequence chunk found: /home/user/notebooks/data/train/fnm_input_seq_parquet_5\n",
      "\tSequence chunk found: /home/user/notebooks/data/train/fnm_input_seq_parquet_6\n",
      "\tSequence chunk found: /home/user/notebooks/data/train/fnm_input_seq_parquet_7\n",
      "\tSequence chunk found: /home/user/notebooks/data/train/fnm_input_seq_parquet_8\n",
      "\tSequence chunk found: /home/user/notebooks/data/train/fnm_input_seq_parquet_9\n",
      "Acquisition numpy: /home/user/notebooks/data/train/fnm_input_acq.npy\n",
      "Sequence numpy: /home/user/notebooks/data/train/fnm_input_seq_{}.npy\n",
      "Index to Sequence Index numpy: /home/user/notebooks/data/train/fnm_input_idx_to_seq.npy\n",
      "processing name: /home/user/notebooks/data/train/fnm_input_seq_parquet_0\n",
      "processing name: /home/user/notebooks/data/train/fnm_input_seq_parquet_1\n",
      "processing name: /home/user/notebooks/data/train/fnm_input_seq_parquet_2\n",
      "processing name: /home/user/notebooks/data/train/fnm_input_seq_parquet_3\n",
      "processing name: /home/user/notebooks/data/train/fnm_input_seq_parquet_4\n",
      "processing name: /home/user/notebooks/data/train/fnm_input_seq_parquet_5\n",
      "processing name: /home/user/notebooks/data/train/fnm_input_seq_parquet_6\n",
      "processing name: /home/user/notebooks/data/train/fnm_input_seq_parquet_7\n",
      "processing name: /home/user/notebooks/data/train/fnm_input_seq_parquet_8\n",
      "processing name: /home/user/notebooks/data/train/fnm_input_seq_parquet_9\n",
      "concatenating lid_to_seq_idx\n"
     ]
    }
   ],
   "source": [
    "convertDataset(data_path = '/home/user/notebooks/data/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: /home/user/notebooks/data/valid\n",
      "Acquistion parquet: /home/user/notebooks/data/valid/fnm_input_acq_parquet\n",
      "Sequence parquet: /home/user/notebooks/data/valid/fnm_input_seq_parquet*\n",
      "\tSequence chunk found: /home/user/notebooks/data/valid/fnm_input_seq_parquet_0\n",
      "Acquisition numpy: /home/user/notebooks/data/valid/fnm_input_acq.npy\n",
      "Sequence numpy: /home/user/notebooks/data/valid/fnm_input_seq_{}.npy\n",
      "Index to Sequence Index numpy: /home/user/notebooks/data/valid/fnm_input_idx_to_seq.npy\n",
      "processing name: /home/user/notebooks/data/valid/fnm_input_seq_parquet_0\n",
      "concatenating lid_to_seq_idx\n"
     ]
    }
   ],
   "source": [
    "convertDataset(data_path = '/home/user/notebooks/data/valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
