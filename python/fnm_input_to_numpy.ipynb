{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pandas in /opt/conda/lib/python3.6/site-packages (0.25.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /opt/conda/lib/python3.6/site-packages (from pandas) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertAcq(acq_filename, includeLoanID = False):\n",
    "    acq = pd.read_parquet(acq_filename).sort_values('loan_id').reset_index(drop=True)\n",
    "    acq = acq.fillna(0)\n",
    "    if includeLoanID:\n",
    "        acq_numpy = acq[['loan_id', 'acq_def_ind', 'state_id', 'purpose_id', 'mi_type_id', \\\n",
    "                    'occupancy_status_id', 'product_type_id', 'property_type_id', \\\n",
    "                        'seller_id', 'zip3_id']].to_numpy()\n",
    "    else:\n",
    "        acq_numpy = acq[[           'acq_def_ind', 'state_id', 'purpose_id', 'mi_type_id', \\\n",
    "            'occupancy_status_id', 'product_type_id', 'property_type_id', \\\n",
    "                'seller_id', 'zip3_id']].to_numpy(dtype=np.int32)\n",
    "    return acq_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertSeq(files, includeLoanID = False):\n",
    "    # mapping of index or loan id to chunk and sequence offsets\n",
    "    lid_to_seq_idx =  []     \n",
    "    seq_numpy = [None] * len(files)\n",
    "    #seq_fnames = { int(files[i].split('/')[-1].split('.')[0].split('_')[-1]): files[i] for i in range(len(files))}\n",
    "\n",
    "    for i, fname in enumerate(files):\n",
    "        print('processing name: {}'.format(fname))\n",
    "        seq = pd.read_parquet(fname)\n",
    "        seq = seq[seq.dlq_adj <= 6 + 12]\n",
    "        seq = seq.sort_values(['loan_id', 'yyyymm']).reset_index(drop=True)\n",
    "        lid = seq.loan_id.to_numpy()\n",
    "        lid_idx = np.concatenate((np.array([0]), np.where(lid[:-1]!=lid[1:])[0]+1, np.array([len(lid)])))\n",
    "\n",
    "        lid_to_seq_idx.append(pd.DataFrame({'loan_id':lid[lid_idx[:-1]], 'chunk_id': i, 'seq_idx_begin':lid_idx[:-1], 'seq_idx_end':lid_idx[1:]}))\n",
    "        \n",
    "        if includeLoanID:\n",
    "            seq_numpy[i] = seq[['loan_id', 'default_1y', 'yyyymm', 'dlq_adj', 'age', 'int_rate', 'current_upb_norm']].to_numpy(dtype=np.float64)\n",
    "        else:\n",
    "            seq_numpy[i] = seq[[           'default_1y', 'yyyymm', 'dlq_adj', 'age', 'int_rate', 'current_upb_norm']].to_numpy(dtype=np.float32)\n",
    "        del seq\n",
    "        gc.collect()\n",
    "    \n",
    "    print('concatenating lid_to_seq_idx')\n",
    "    lid_to_seq_idx = pd.concat(lid_to_seq_idx).sort_values('loan_id')\n",
    "    \n",
    "    return lid_to_seq_idx, seq_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDataset(data_path):\n",
    "    acquistion_fname = '/fnm_input_acq_parquet'\n",
    "    sequence_fname = '/fnm_input_seq_parquet*'\n",
    "\n",
    "    acquisition_nname = '/fnm_input_acq.npy'\n",
    "    sequence_nname = '/fnm_input_seq_{}.npy'\n",
    "    idx_to_seq_nname = '/fnm_input_idx_to_seq.npy'\n",
    "\n",
    "    print('Data path: {}'.format(data_path))\n",
    "    print('Acquistion parquet: {}'.format(data_path + acquistion_fname))\n",
    "    print('Sequence parquet: {}'.format(data_path + sequence_fname))\n",
    "\n",
    "    seq_files = sorted([f for f in glob.glob(data_path + sequence_fname, recursive=False)])\n",
    "    for f in seq_files:\n",
    "        print('\\tSequence chunk found: {}'.format(f))\n",
    "\n",
    "    print('Acquisition numpy: {}'.format(data_path + acquisition_nname))\n",
    "    print('Sequence numpy: {}'.format(data_path + sequence_nname))\n",
    "    print('Index to Sequence Index numpy: {}'.format(data_path + idx_to_seq_nname))\n",
    "    \n",
    "    acq_numpy = convertAcq(data_path + acquistion_fname)\n",
    "    lid_to_seq_idx, seq_numpy = convertSeq(seq_files, includeLoanID = False)\n",
    "    idx_to_seq = lid_to_seq_idx[['chunk_id', 'seq_idx_begin', 'seq_idx_end', 'loan_id']].to_numpy()\n",
    "    \n",
    "    np.save(data_path + acquisition_nname, acq_numpy, allow_pickle=False, fix_imports=False)\n",
    "    np.save(data_path + idx_to_seq_nname, idx_to_seq, allow_pickle=False, fix_imports=False)\n",
    "    for chunk_idx, seq_numpy_chunk in enumerate(seq_numpy):\n",
    "        np.save(data_path + sequence_nname.format(chunk_idx), seq_numpy[chunk_idx], allow_pickle=False, fix_imports=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: /home/user/notebooks/data/test/parquet\n",
      "Acquistion parquet: /home/user/notebooks/data/test/parquet/fnm_input_acq_parquet\n",
      "Sequence parquet: /home/user/notebooks/data/test/parquet/fnm_input_seq_parquet*\n",
      "\tSequence chunk found: /home/user/notebooks/data/test/parquet/fnm_input_seq_parquet_0.90\n",
      "\tSequence chunk found: /home/user/notebooks/data/test/parquet/fnm_input_seq_parquet_0.92\n",
      "\tSequence chunk found: /home/user/notebooks/data/test/parquet/fnm_input_seq_parquet_0.94\n",
      "\tSequence chunk found: /home/user/notebooks/data/test/parquet/fnm_input_seq_parquet_0.96\n",
      "\tSequence chunk found: /home/user/notebooks/data/test/parquet/fnm_input_seq_parquet_0.98\n",
      "Acquisition numpy: /home/user/notebooks/data/test/parquet/fnm_input_acq.npy\n",
      "Sequence numpy: /home/user/notebooks/data/test/parquet/fnm_input_seq_{}.npy\n",
      "Index to Sequence Index numpy: /home/user/notebooks/data/test/parquet/fnm_input_idx_to_seq.npy\n",
      "processing name: /home/user/notebooks/data/test/parquet/fnm_input_seq_parquet_0.98\n",
      "concatenating lid_to_seq_idx\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be saved when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-385b58ad86a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconvertDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/user/notebooks/data/test/parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-7d7157c0fd28>\u001b[0m in \u001b[0;36mconvertDataset\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0midx_to_seq_nname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_to_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfix_imports\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_numpy_chunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msequence_nname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_numpy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchunk_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfix_imports\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[0;32m--> 536\u001b[0;31m                            pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    537\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0;31m# pickle protocol.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m             raise ValueError(\"Object arrays cannot be saved when \"\n\u001b[0m\u001b[1;32m    630\u001b[0m                              \"allow_pickle=False\")\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Object arrays cannot be saved when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "convertDataset(data_path = '/home/user/notebooks/data/test/parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: /home/user/notebooks/data/train\n",
      "Acquistion parquet: /home/user/notebooks/data/train/fnm_input_acq_parquet\n",
      "Sequence parquet: /home/user/notebooks/data/train/fnm_input_seq_parquet*\n",
      "\tSequence chunk found: /home/user/notebooks/data/train/fnm_input_seq_parquet_0\n",
      "\tSequence chunk found: /home/user/notebooks/data/train/fnm_input_seq_parquet_1\n",
      "\tSequence chunk found: /home/user/notebooks/data/train/fnm_input_seq_parquet_2\n",
      "\tSequence chunk found: /home/user/notebooks/data/train/fnm_input_seq_parquet_3\n",
      "\tSequence chunk found: /home/user/notebooks/data/train/fnm_input_seq_parquet_4\n",
      "\tSequence chunk found: /home/user/notebooks/data/train/fnm_input_seq_parquet_5\n",
      "\tSequence chunk found: /home/user/notebooks/data/train/fnm_input_seq_parquet_6\n",
      "\tSequence chunk found: /home/user/notebooks/data/train/fnm_input_seq_parquet_7\n",
      "\tSequence chunk found: /home/user/notebooks/data/train/fnm_input_seq_parquet_8\n",
      "\tSequence chunk found: /home/user/notebooks/data/train/fnm_input_seq_parquet_9\n",
      "Acquisition numpy: /home/user/notebooks/data/train/fnm_input_acq.npy\n",
      "Sequence numpy: /home/user/notebooks/data/train/fnm_input_seq_{}.npy\n",
      "Index to Sequence Index numpy: /home/user/notebooks/data/train/fnm_input_idx_to_seq.npy\n",
      "processing name: /home/user/notebooks/data/train/fnm_input_seq_parquet_0\n",
      "processing name: /home/user/notebooks/data/train/fnm_input_seq_parquet_1\n",
      "processing name: /home/user/notebooks/data/train/fnm_input_seq_parquet_2\n",
      "processing name: /home/user/notebooks/data/train/fnm_input_seq_parquet_3\n",
      "processing name: /home/user/notebooks/data/train/fnm_input_seq_parquet_4\n",
      "processing name: /home/user/notebooks/data/train/fnm_input_seq_parquet_5\n",
      "processing name: /home/user/notebooks/data/train/fnm_input_seq_parquet_6\n",
      "processing name: /home/user/notebooks/data/train/fnm_input_seq_parquet_7\n",
      "processing name: /home/user/notebooks/data/train/fnm_input_seq_parquet_8\n",
      "processing name: /home/user/notebooks/data/train/fnm_input_seq_parquet_9\n",
      "concatenating lid_to_seq_idx\n"
     ]
    }
   ],
   "source": [
    "convertDataset(data_path = '/home/user/notebooks/data/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: /home/user/notebooks/data/valid\n",
      "Acquistion parquet: /home/user/notebooks/data/valid/fnm_input_acq_parquet\n",
      "Sequence parquet: /home/user/notebooks/data/valid/fnm_input_seq_parquet*\n",
      "\tSequence chunk found: /home/user/notebooks/data/valid/fnm_input_seq_parquet_0\n",
      "Acquisition numpy: /home/user/notebooks/data/valid/fnm_input_acq.npy\n",
      "Sequence numpy: /home/user/notebooks/data/valid/fnm_input_seq_{}.npy\n",
      "Index to Sequence Index numpy: /home/user/notebooks/data/valid/fnm_input_idx_to_seq.npy\n",
      "processing name: /home/user/notebooks/data/valid/fnm_input_seq_parquet_0\n",
      "concatenating lid_to_seq_idx\n"
     ]
    }
   ],
   "source": [
    "convertDataset(data_path = '/home/user/notebooks/data/valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
